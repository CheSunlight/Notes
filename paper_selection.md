# Paper Selection

## Avoid Overfitting

### Regularization

1. Mei, J. P., Qiu, W., Chen, D., Yan, R., & Fan, J. (2023). Output Regularization With Cluster-Based Soft Targets. *IEEE Transactions on Neural Networks and Learning Systems*.
    - Using clustering-based soft targets for output regularization.
2. Zeng, Z., Peng, Q., Mou, X., Wang, Y., & Li, R. (2023). Graph Neural Networks With High-Order Polynomial Spectral Filters. *IEEE Transactions on Neural Networks and Learning Systems*.
   - Using dimensionality reduction of domain and spectral filter with a forgetting factor to avoid overfitting in spectral GNN.

### Data Augmentation Methods

1. Cao, J., Luo, M., Yu, J., Yang, M. H., & He, R. (2022). ScoreMix: A Scalable Augmentation Strategy for Training GANs with Limited Data. *IEEE Transactions on Pattern Analysis and Machine Intelligence*.
   - Tackle the problem that GAN would overfit due to limited data.

### Improved Loss Function

1. Zhou, X., Liu, X., Zhai, D., Jiang, J., & Ji, X. (2023). Asymmetric Loss Functions for Noise-Tolerant Learning: Theory and Applications. *IEEE Transactions on Pattern Analysis and Machine Intelligence*.
    - Use asymmetric loss functions to tolerate noisy labels.

### Sample Reweighting

1. Shu, J., Yuan, X., Meng, D., & Xu, Z. (2023). Cmw-net: Learning a class-aware sample weighting mapping for robust deep learning. *IEEE Transactions on Pattern Analysis and Machine Intelligence*.
    - Use Meta Learning method to learn the sample weights and use soft targets to tackle the overfitting problem induced by class imbalance and noisy labels.

## Nonstationary Environment (Concept Drift)

1. Wu, X., Jiang, B., Wang, X., Ban, T., & Chen, H. (2023). Feature Selection in the Data Stream Based on Incremental Markov Boundary Learning. *IEEE Transactions on Neural Networks and Learning Systems*.
    - Incorporating prior knowledge (old data) into MB (Markov Boundary) learning to get the minimal useful feature set to tackle the data shift problem.

2. Fedeli, F., Metelli, A. M., Trov√≤, F., & Restelli, M. (2023). IWDA: Importance Weighting for Drift Adaptation in Streaming Supervised Learning Problems. *IEEE Transactions on Neural Networks and Learning Systems*.
   - Using importance-weighted empirical risk minimization (a reweighting method) to tackle the distribution shift problem.

3. Huang, F., Zhang, S., & Zheng, W. X. (2023). Bayesian-Learning-Based Diffusion Least Mean Square Algorithms Over Networks. *IEEE Transactions on Neural Networks and Learning Systems*.
   - Using Gaussian state-space model-based Bayesian method to improve conventional DLMS (diffusion least mean square) methods to solve time-varying unknown parameter problem.

## Which Test Assets?

How to choose the most suitable test assets to test factors?

### Clustering


### Maximal Predictable Portfolio
