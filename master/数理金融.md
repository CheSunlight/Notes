---
puppeteer:
    printBackground: true
---

# 数理金融
- 2022春季
- 杨招军

[toc]

<div STYLE='page-break-after: always;'></div>

### Discrete-Time Dynamic Programming

- **Basic Structure**
  - State Transfer System
    $$
    x_{k+1} = f_k(x_k,\ u_k),\ k=0,\ 1,\ \cdots,\ N-1
    $$
    - $k$: discrete-time
    - $x_k$: state
    - $u_k$: control (dicision to be selected), $u_k \in U$ with a control set $U$
    - $f_k$: rule (a.k.a. state transfer function)
    - $N$: horizon
  - Policy and Value Functional
      - Policy $\pi=(\mu_0,\ \mu_1,\ \cdots,\ \mu_{N-1})$ maps states into controls, i.e., $u_k=\mu_k(x_k)$ (can be understood as strategy).
      - ==Value functional== of $\pi$ starting at $x_0$ is 
      $$
      J_{\pi}(x_0) = g_{N}(x_{N}) + \sum\limits_{k=0}^{N-1} g_k(x_k,\ u_k)
      $$where $g(\cdot)$ is the payoff of control under specific discete-time and state.
      - ==Value function== $V_0(x_0)=\max_{\pi}\ J_{\pi}(x_0)$ and the corresponding policy is the optimal policy $\pi^{*}$
      > Difference between functional and function: the independent variable of a functional is function.
      The independent variable of $J_{\pi}(x_0)$ is $\pi$, whose components are functions.
  - Example
    - The Shortest Path Problem
    ![](image/2022-02-16-12-12-32.png)
    - $x_0=A,\ x_1 \in \{B_1,\ B_2\},\ \cdots,\ x_6=G$
    - $g_5(F_1,\ G)=-4,\ g_4(E_1,\ F_1)=-3,\ \cdots$
<br>

- **Principle of Optimality**
  - ==Definition==
  From any point on an optimal trajectory, the remaining trajectory is optimal for the corresponding problem initiated at that point.
  $$
  V_k(x_k) = \underset{u_k \in U}{\max}\ \left[ g_k(x_k,\ u_k) + V_{k+1}(f_k(x_k,\ u_k)) \right] 
  $$
  - Developed by Richard Bellman (1953).
  - To solve the optimality is to solve all the tail subproblem. For example, consider the shortest path problem above, we start at $G$ and then go backwards to get optimal policies at each state.
  $$
  V_6(G) = 0,\ V_5(F_1) = -4,\ V_5(F_2) = -3 \\
  \ \\
  \begin{aligned}
   g_4(E_1,\ F_1) + V_5(F_1) &= -7,\ & g_4(E_1,\ F_2) + V_5(F_2) &= -8 \implies & V_4(E_1) &= -7 \\
   g_4(E_2,\ F_1) + V_5(F_1) &= -9,\ & g_4(E_2,\ F_2) + V_5(F_2) &= -5 \implies & V_4(E_2) &= -5 \\
   g_4(E_3,\ F_1) + V_5(F_1) &= -10,\ & g_4(E_3,\ F_2) + V_5(F_2) &= -9 \implies & V_4(E_3) &= -9 \\
   \ \\
   g_3(D_1,\ E_1) + V_4(E_1) &= -9,\ & g_3(D_1,\ E_2) + V_4(E_2) &= -7 \implies & V_3(D_1) &= -7 \\
   g_3(D_2,\ E_2) + V_4(E_2) &= -6,\ & g_3(D_2,\ E_3) + V_4(E_3) &= -11 \implies & V_3(D_2) &= -6 \\
   g_3(D_3,\ E_2) + V_4(E_2) &= -8,\ & g_3(D_3,\ E_3) + V_4(E_3) &= -12 \implies & V_3(D_3) &= -8 \\
   \ \\
   g_2(C_1,\ D_1) + V_3(D_1) &= -13,\ & g_2(C_1,\ D_2) + V_3(D_2) &= -14 \implies & V_2(C_1) &= -13 \\
   g_2(C_2,\ D_1) + V_3(D_1) &= -10,\ & g_2(C_2,\ D_2) + V_3(D_2) &= -11 \implies & V_2(C_2) &= -10 \\
   g_2(C_3,\ D_2) + V_3(D_2) &= -9,\ & g_2(C_3,\ D_3) + V_3(D_3) &= -11 \implies & V_2(C_3) &= -9 \\
   g_2(C_4,\ D_2) + V_3(D_2) &= -14,\ & g_2(C_4,\ D_3) + V_3(D_3) &= -12 \implies & V_2(C_4) &= -12 \\
  \end{aligned}\\
   \ \\
  \begin{aligned}
   g_1(B_1,\ C_1) + V_2(C_1) &= -14,\ & g_1(B_1,\ C_2) + V_2(C_2) &= -13,\ & g_1(B_1,\ C_3) + V_2(C_3) &= -15 \implies & V_2(B_1) &= -13 \\
   g_1(B_2,\ C_2) + V_2(C_2) &= -18,\ & g_1(B_2,\ C_3) + V_2(C_3) &= -16,\ & g_1(B_2,\ C_4) + V_2(C_4) &= -18 \implies & V_2(B_2) &= -16 \\
  \end{aligned}\\
   \ \\
   g_0(A,\ B_1) + V_1(B_1) = -18,\ g_0(A,\ B_2) + V_1(B_2) = -19 \implies V_0(A) = -18
  $$which means the optimal path is $A\to B_1\to C_2\to D_1\to E_2\to F_2\to G$.
<br>

- **Stochastic Dynamic Programming**
  - State $x_{k+1}=f_k(x_k,\ u_k,\ z_k)$ where $z_k$ is a random variable with probability distribution $\mathrm{P}_k(\cdot|x_k,\ u_k)$
  - Value functional becomes 
  $$
  \mathrm{E}\left( g_N(x_N)+\sum\limits_{k=0}^{N-1} g_k(x_k,\ u_k,\ z_k) \right) 
  $$
  - Formula of Principle of Optimality becomes 
  $$
  V_k(x_k) = \underset{u_k \in U}{\max}\ \mathrm{E}\left[ g_k(x_k,\ u_k,\ z_k) + V_{k+1}(f_k(x_k,\ u_k,\ z_k)) \right] 
  $$

### Continuous-Time Deterministic Dynamic Programming
- **Basic Structure**
  - Control System
  $$
\begin{cases}
  \frac{\mathrm{d}x}{\mathrm{d}t} = b(t,\ x(t),\ u(t)),& t \in [0,\ T] \\
  x(0) = x_0
\end{cases}
$$where the initial value $x_0 \in \mathbb{R}$, the control $u: [0,\ T] \to U$ with a metric space $U$ and the map $b: [0,\ T]\times \mathbb{R}\times U \to \mathbb{R}$.
Consider $s \in [0,\ T)$ and $y \in \mathbb{R}$, the control system becomes
$$
\begin{cases}
  \frac{\mathrm{d}x}{\mathrm{d}t} = b(t,\ x(t),\ u(t)),& t \in [s,\ T] \\
  x(s) = y
\end{cases}
$$
  - Value Functional
  $$
  J(s,\ y;\ u(\cdot)) = h(x(T)) + \int_s^{T} f(t,\ x(t),\ u(t))~\mathrm{d}t
  $$for some given payoff maps $f$ and $h$.
  - Value Function
  $$
  V(s,\ y) =
  \begin{cases}
     \underset{u \in U}{\mathrm{sup}}\ J(s,\ y;\ u(\cdot)),& s \in [0,\ T) \\
     h(y),& s=T
  \end{cases}
  $$

- **Principle of Optimality**
![](image/2022-02-23-23-28-56.png)
The equation above is called the ==dynamic programming equation==.
  - If the value function $V \in \mathbb{C}^{1}([0,\ T]\times \mathbb{R})$ (the first order derivatives are continuous), then $V$ satisfies the ==HJB (Hamilton-Jacobi-Bellman) equation==: 
  $$
  \begin{cases}
    V_t + \underset{u \in U}{\mathrm{sup}}\ [b(t,\ x,\ u)V_x + f(t,\ x,\ u)] = 0 \\
    V(t=T,\ x) = h(x)
  \end{cases}
  $$where $x(t)$ and $u(t)$ are simply written as $x$ and $u$.
    - ==Proof==
      Fix a $u \in U$. By the principle of optimality, we have
      $$
       \int_{s}^{\hat{s}} f(t,\ x,\ u)  ~\mathrm{d}t + V(\hat{s},\ x(\hat{s})) \leqslant V(s,\ y)
      $$Let $\hat{s} \to s$, then we can write 
      $$
       \int_{s}^{\hat{s}} f(t,\ x,\ u)  ~\mathrm{d}t = (\hat{s}-s)f(s,\ x(s),\ u(s)) + o(\hat{s}-s) \implies \frac{\int_{s}^{\hat{s}} f(t,\ x,\ u)  ~\mathrm{d}t}{\hat{s}-s} = f(s,\ x(s),\ u(s))
      $$and 
      $$
       V(\hat{s},\ x(\hat{s})) = V(s,\ x(s)) +V_t|_{t=s}(\hat{s}-s)+o(\hat{s}-s)+V_x|_{x=x(s)}(x(\hat{s})-x(s))+o(x(\hat{s})-x(s)) \\
       \implies \frac{V(\hat{s},\ x(\hat{s}))-V(s,\ x(s))}{\hat{s}-s} = V_t|_{t=s} + V_x|_{x=x(s)}b(s,\ x(s),\ u(s))
      $$by Taylor expansion.
      > Note that when $\hat{s} \to s$, $\frac{o(x(\hat{s})-x(s))}{\hat{s}-s}=\frac{o(x(\hat{s})-x(s))}{x(\hat{s})-x(s)}\frac{x(\hat{s})-x(s)}{\hat{s}-s}=\frac{o(x(\hat{s})-x(s))}{x(\hat{s})-x(s)}b(s,\ x(s),\ u(s))=0$. 

      Thus, we have 
      $$
      f(s,\ x(s),\ u(s)) + V_t|_{t=s} + V_x|_{x=x(s)}b(s,\ x(s),\ u(s)) \leqslant 0
      $$Then for any $u \in U$, we obtain 
      $$
      V_t|_{t=s} + \underset{u \in U}{\mathrm{sup}}[ f(s,\ x(s),\ u(s)) + V_x|_{x=x(s)}b(s,\ x(s),\ u(s)) ] \leqslant 0
      $$On the other hand, by the definition of supremum, $\forall \varepsilon>0$ with $\hat{s}-s>0$ small enough, $\exists u \in U$ s.t. 
      $$
      V(s,\ y) - \varepsilon(\hat{s}-s) \leqslant \int_{s}^{\hat{s}} f(t,\ x,\ u)  ~\mathrm{d}t + V(\hat{s},\ x(\hat{s}))
      $$Using the same method above, this yeilds
      $$
      f(s,\ x(s),\ u(s)) + V_t|_{t=s} + V_x|_{x=x(s)}b(s,\ x(s),\ u(s)) \geqslant -\varepsilon
      $$which means 
      $$
      V_t|_{t=s} + \underset{u \in U}{\mathrm{sup}}[ f(s,\ x(s),\ u(s)) + V_x|_{x=x(s)}b(s,\ x(s),\ u(s)) ] \geqslant 0
      $$Hence, we finish the proof.
  - Example
    - The neoclassical production function is 
    $$
    Y = F(K,\ L) 
    $$where $Y$ is output, $K$ and $L$ are time-varying capital input and constant labor input, respectively, and $F(\cdot,\ \cdot)$ is homogeneous of degree one, i.e., $F(\alpha K,\ \alpha L)=\alpha F(K,\ L),\ \forall \alpha>0$.
    Thus, if we take labor $L$ to be fixed, per capita output can be expressed as 
    $$
    y = \frac{Y}{L} = F\left( \frac{K}{L}, 1 \right)  := f(k)
    $$where $k=\frac{K}{L}$.
    - The well-known macroeconomic identities show that 
    $$
    \begin{cases}
      I = Y - C \\
      \mathrm{d}K = I \mathrm{d}t - \delta K \mathrm{d}t
    \end{cases}
    $$where $I$ represents investment, $C$ is total consumption and $\delta$ is the rate of depreciation of capital.
    Let per capita consumption $c=\frac{C}{L}$, we obtain 
    $$
    \frac{\mathrm{d}k}{\mathrm{d}t} = \frac{1}{L}\frac{\mathrm{d}K}{\mathrm{d}t} = \frac{I-\delta K}{L} = \frac{Y}{L} - \frac{C}{L} - \delta \frac{K}{L} = f(k) - c - \delta k
    $$which means the control system can be written as 
    $$
    \begin{cases}
      \frac{\mathrm{d}k}{\mathrm{d}t} = f(k) - c - \delta k\\
      k(0) = k_0
    \end{cases}
    $$given initial per capita capital $k_0$.
    - Ramsey (1928) proposed the idea of a bliss point, an accumulation point of a sequence of consumption decisions representing the nonattainable, ideal consumption goal of the consumer. In math form, we write the bliss point 
    $$
    B=\underset{c\geqslant 0}{\mathrm{sup}}\ U(c)>0
    $$where $U(c)$ is the utility experienced as a result of per capita consumption $c$.
    Consumers want to minimize the gap between utility and the bliss point, so the value functional of the control system can be written as 
    $$
    J(0,\ k_0;\ c(\cdot)) = \int_{0}^{\infty} U(c)-B ~\mathrm{d}t 
    $$and the value function 
    $$
    V(s,\ k) = \underset{c}{\mathrm{sup}}\ \int_{0}^{\infty} U(c)-B ~\mathrm{d}t = V(k)
    $$is time-homogeneous.
    - Assume $U(c) = B - e^{-\lambda c}$ with constant $\lambda>0$.

<br>
